#!/usr/bin/env python
# vim:fileencoding=utf-8

from calibre.web.feeds.news import BasicNewsRecipe
import json
import os
import tempfile

try:
    from urllib.request import Request, urlopen
    from urllib.error import HTTPError, URLError
except ImportError:
    from urllib2 import Request, urlopen, HTTPError, URLError


class ReadwiseReader(BasicNewsRecipe):
    title = 'Readwise Reader'
    __author__ = 'Ryan'
    description = 'Fetch articles from your Readwise Reader inbox'
    language = 'en'
    encoding = 'utf-8'
    no_stylesheets = True
    remove_javascript = True
    ignore_duplicate_articles = {'url'}
    needs_subscription = True
    auto_cleanup = False

    # Default settings
    max_articles_per_feed = 20
    oldest_article = 365

    # Recipe-specific options for user configuration
    recipe_specific_options = {
        'max_articles': {
            'short': 'Maximum articles',
            'long': 'Maximum number of articles to download (default: 20)',
            'default': '20'
        }
    }

    # Readwise API configuration
    API_BASE = 'https://readwise.io/api/v3'

    def get_cover_url(self):
        if hasattr(self, '_cover_path') and self._cover_path:
            return 'file://' + self._cover_path
        return None

    def _get_token(self):
        """Get the API token from Calibre's password field."""
        if not self.password:
            raise ValueError(
                'Readwise API token is required. '
                'Enter your token in the password field. '
                'Get your token at: https://readwise.io/access_token'
            )
        return self.password.strip()

    def _api_request(self, endpoint, params=None):
        """Make an authenticated request to the Readwise API."""
        token = self._get_token()
        url = f'{self.API_BASE}/{endpoint}'

        if params:
            query_string = '&'.join(f'{k}={v}' for k, v in params.items())
            url = f'{url}?{query_string}'

        request = Request(url)
        request.add_header('Authorization', f'Token {token}')
        request.add_header('Accept', 'application/json')

        try:
            response = urlopen(request, timeout=30)
            return json.loads(response.read().decode('utf-8'))
        except HTTPError as e:
            if e.code == 401:
                raise ValueError(
                    'Invalid API token. Please check your Readwise API token. '
                    'Get a valid token at: https://readwise.io/access_token'
                )
            elif e.code == 429:
                raise ValueError(
                    'Rate limited by Readwise API. Please try again later.'
                )
            else:
                raise ValueError(f'API request failed: HTTP {e.code}')
        except URLError as e:
            raise ValueError(f'Network error: {e.reason}')

    def _fetch_articles(self, max_count):
        """Fetch articles from Readwise Reader inbox."""
        articles = []
        cursor = None

        while len(articles) < max_count:
            params = {
                'location': 'new',
                'category': 'article',
                'withHtmlContent': 'true',
                'pageSize': min(100, max_count - len(articles))
            }

            if cursor:
                params['pageCursor'] = cursor

            data = self._api_request('list/', params)
            results = data.get('results', [])

            if not results:
                break

            articles.extend(results)
            cursor = data.get('nextPageCursor')

            if not cursor:
                break

        return articles[:max_count]

    def _generate_cover(self, image_urls, article_count):
        """Generate a cover image with article thumbnails and text overlay."""
        from io import BytesIO
        from datetime import datetime

        try:
            from PIL import Image, ImageDraw, ImageFont
        except ImportError:
            self.log.warn('PIL not available, skipping cover generation')
            return None

        COVER_WIDTH, COVER_HEIGHT = 600, 800
        GRID_COLS, GRID_ROWS = 2, 2
        CELL_WIDTH = COVER_WIDTH // GRID_COLS
        CELL_HEIGHT = COVER_HEIGHT // GRID_ROWS

        # Create base image with dark background
        cover = Image.new('RGB', (COVER_WIDTH, COVER_HEIGHT), (26, 26, 26))

        # Download and place images in grid
        images_placed = 0
        for idx, url in enumerate(image_urls[:4]):
            if not url:
                continue
            try:
                self.log(f'Downloading cover image {idx + 1}: {url[:50]}...')
                request = Request(url)
                request.add_header('User-Agent', 'Mozilla/5.0')
                response = urlopen(request, timeout=10)
                img_data = response.read()
                img = Image.open(BytesIO(img_data))

                # Convert to RGB if necessary
                if img.mode in ('RGBA', 'P'):
                    img = img.convert('RGB')

                # Resize to fill cell while maintaining aspect ratio, then crop
                img_ratio = img.width / img.height
                cell_ratio = CELL_WIDTH / CELL_HEIGHT

                if img_ratio > cell_ratio:
                    # Image is wider - fit height, crop width
                    new_height = CELL_HEIGHT
                    new_width = int(new_height * img_ratio)
                else:
                    # Image is taller - fit width, crop height
                    new_width = CELL_WIDTH
                    new_height = int(new_width / img_ratio)

                img = img.resize((new_width, new_height), Image.LANCZOS)

                # Crop to cell size from center
                left = (new_width - CELL_WIDTH) // 2
                top = (new_height - CELL_HEIGHT) // 2
                img = img.crop((left, top, left + CELL_WIDTH, top + CELL_HEIGHT))

                # Calculate position in grid
                row = idx // GRID_COLS
                col = idx % GRID_COLS
                x = col * CELL_WIDTH
                y = row * CELL_HEIGHT

                cover.paste(img, (x, y))
                images_placed += 1

            except Exception as e:
                self.log.warn(f'Failed to download image {url}: {e}')
                continue

        self.log(f'Placed {images_placed} images in cover grid')

        # Create overlay
        overlay = Image.new('RGBA', (COVER_WIDTH, COVER_HEIGHT), (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)

        # Semi-transparent overlay at bottom
        overlay_height = 140
        draw.rectangle(
            [(0, COVER_HEIGHT - overlay_height), (COVER_WIDTH, COVER_HEIGHT)],
            fill=(0, 0, 0, 200)
        )

        # Try to use a nice font, fall back to default
        try:
            title_font = ImageFont.truetype('/System/Library/Fonts/Helvetica.ttc', 32)
            date_font = ImageFont.truetype('/System/Library/Fonts/Helvetica.ttc', 24)
            count_font = ImageFont.truetype('/System/Library/Fonts/Helvetica.ttc', 20)
        except (IOError, OSError):
            try:
                title_font = ImageFont.truetype('arial.ttf', 32)
                date_font = ImageFont.truetype('arial.ttf', 24)
                count_font = ImageFont.truetype('arial.ttf', 20)
            except (IOError, OSError):
                title_font = ImageFont.load_default()
                date_font = title_font
                count_font = title_font

        # Draw text
        text_x = 20
        base_y = COVER_HEIGHT - overlay_height + 15

        draw.text((text_x, base_y), 'Readwise Reader', font=title_font, fill='white')
        draw.text((text_x, base_y + 40), datetime.now().strftime('%B %d, %Y'), font=date_font, fill=(200, 200, 200))
        draw.text((text_x, base_y + 75), f'{article_count} Articles', font=count_font, fill=(170, 170, 170))

        # Composite overlay onto cover
        cover = cover.convert('RGBA')
        cover = Image.alpha_composite(cover, overlay)
        cover = cover.convert('RGB')

        # Save to temp file
        cover_path = os.path.join(self._temp_dir, 'cover.jpg')
        cover.save(cover_path, 'JPEG', quality=90)
        self.log(f'Generated cover image: {cover_path}')

        return cover_path

    def _build_html(self, content, title, author, site_name=None, reading_time=None, published_date=None, summary=None):
        """Build a complete HTML document from article content."""
        from html import escape
        safe_title = escape(title) if title else 'Untitled'

        # Build metadata lines
        meta_parts = []
        if site_name:
            meta_parts.append(escape(site_name))
        if author:
            meta_parts.append(f'By {escape(author)}')

        meta_line = ''
        if meta_parts:
            meta_line = f'<p class="meta">{" · ".join(meta_parts)}</p>'

        # Build secondary metadata (reading time, published date)
        details_parts = []
        if reading_time:
            details_parts.append(f'{reading_time} min read')
        if published_date:
            # Format date nicely if it's an ISO string
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                formatted_date = dt.strftime('%B %d, %Y')
                details_parts.append(formatted_date)
            except (ValueError, AttributeError):
                details_parts.append(escape(str(published_date)))

        details_line = ''
        if details_parts:
            details_line = f'<p class="details">{" · ".join(details_parts)}</p>'

        # Summary/description
        summary_line = ''
        if summary:
            summary_line = f'<p class="summary"><em>{escape(summary)}</em></p>'

        return f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{safe_title}</title>
</head>
<body>
    <article>
        <h1>{safe_title}</h1>
        {meta_line}
        {details_line}
        {summary_line}
        <hr>
        <div class="content">
            {content}
        </div>
    </article>
</body>
</html>'''

    def parse_index(self):
        """Parse the Readwise Reader API and return article index."""
        # Get max articles from recipe options
        max_articles = 20
        if hasattr(self, 'recipe_specific_options'):
            try:
                opt_value = self.get_recipe_specific_option('max_articles')
                if opt_value:
                    max_articles = int(opt_value)
            except (ValueError, AttributeError):
                pass

        self.log(f'Fetching up to {max_articles} articles from Readwise Reader inbox...')

        try:
            articles = self._fetch_articles(max_articles)
        except ValueError as e:
            self.log.error(str(e))
            raise

        if not articles:
            self.log.warn('No articles found in your Readwise Reader inbox.')
            return []

        self.log(f'Found {len(articles)} articles')

        # Sort by saved date (most recent first)
        articles.sort(key=lambda x: x.get('saved_at', x.get('created_at', '')), reverse=True)

        # Create temp directory for article HTML files
        self._temp_dir = tempfile.mkdtemp(prefix='readwise_reader_')
        self.log(f'Using temp directory: {self._temp_dir}')

        # Collect image URLs for cover generation
        image_urls = [a.get('image_url') for a in articles if a.get('image_url')]
        self.log(f'Found {len(image_urls)} articles with images for cover')

        # Generate cover image
        if image_urls:
            cover_path = self._generate_cover(image_urls, len(articles))
            if cover_path:
                self._cover_path = cover_path

        # Build the article index
        feed_articles = []
        for idx, article in enumerate(articles):
            title = article.get('title') or 'Untitled'
            url = article.get('source_url') or article.get('url', '')
            author = article.get('author', '')
            summary = article.get('summary', '')
            html_content = article.get('html_content', '')
            site_name = article.get('site_name', '')
            reading_time = article.get('reading_progress', {}).get('time_remaining') if isinstance(article.get('reading_progress'), dict) else None
            # Try reading_time field directly if available
            if not reading_time:
                reading_time = article.get('reading_time')
            published_date = article.get('published_date', '')

            if not html_content:
                self.log.warn(f'Skipping "{title}" - no HTML content available')
                continue

            # Write HTML to temp file
            html = self._build_html(
                html_content, title, author,
                site_name=site_name,
                reading_time=reading_time,
                published_date=published_date,
                summary=summary
            )
            filename = f'article_{idx}.html'
            filepath = os.path.join(self._temp_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html)

            self.log(f'Saved article: {title}')

            feed_articles.append({
                'title': title,
                'url': 'file://' + filepath,
                'author': author,
                'description': summary,
            })

        if not feed_articles:
            self.log.warn('No articles with content found.')
            return []

        self.log(f'Prepared {len(feed_articles)} articles for download')
        return [('Readwise Reader Inbox', feed_articles)]

    def preprocess_html(self, soup):
        """Clean up the HTML before conversion."""
        # Remove any script tags that might have slipped through
        for script in soup.find_all('script'):
            script.decompose()

        # Remove style tags
        for style in soup.find_all('style'):
            style.decompose()

        return soup
