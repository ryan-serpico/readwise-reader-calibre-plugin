#!/usr/bin/env python
# vim:fileencoding=utf-8

from calibre.web.feeds.news import BasicNewsRecipe
import json
import os
import tempfile

try:
    from urllib.request import Request, urlopen
    from urllib.error import HTTPError, URLError
except ImportError:
    from urllib2 import Request, urlopen, HTTPError, URLError


class ReadwiseReader(BasicNewsRecipe):
    title = 'Readwise Reader'
    __author__ = 'Ryan'
    description = 'Fetch articles from your Readwise Reader inbox'
    language = 'en'
    encoding = 'utf-8'
    no_stylesheets = True
    remove_javascript = True
    ignore_duplicate_articles = {'url'}
    needs_subscription = True
    auto_cleanup = False

    # Default settings
    max_articles_per_feed = 20
    oldest_article = 365

    # Recipe-specific options for user configuration
    recipe_specific_options = {
        'max_articles': {
            'short': 'Maximum articles',
            'long': 'Maximum number of articles to download (default: 20)',
            'default': '20'
        }
    }

    # Readwise API configuration
    API_BASE = 'https://readwise.io/api/v3'

    def get_cover_url(self):
        return None

    def _get_token(self):
        """Get the API token from Calibre's password field."""
        if not self.password:
            raise ValueError(
                'Readwise API token is required. '
                'Enter your token in the password field. '
                'Get your token at: https://readwise.io/access_token'
            )
        return self.password.strip()

    def _api_request(self, endpoint, params=None):
        """Make an authenticated request to the Readwise API."""
        token = self._get_token()
        url = f'{self.API_BASE}/{endpoint}'

        if params:
            query_string = '&'.join(f'{k}={v}' for k, v in params.items())
            url = f'{url}?{query_string}'

        request = Request(url)
        request.add_header('Authorization', f'Token {token}')
        request.add_header('Accept', 'application/json')

        try:
            response = urlopen(request, timeout=30)
            return json.loads(response.read().decode('utf-8'))
        except HTTPError as e:
            if e.code == 401:
                raise ValueError(
                    'Invalid API token. Please check your Readwise API token. '
                    'Get a valid token at: https://readwise.io/access_token'
                )
            elif e.code == 429:
                raise ValueError(
                    'Rate limited by Readwise API. Please try again later.'
                )
            else:
                raise ValueError(f'API request failed: HTTP {e.code}')
        except URLError as e:
            raise ValueError(f'Network error: {e.reason}')

    def _fetch_articles(self, max_count):
        """Fetch articles from Readwise Reader inbox."""
        articles = []
        cursor = None

        while len(articles) < max_count:
            params = {
                'location': 'new',
                'category': 'article',
                'withHtmlContent': 'true',
                'pageSize': min(100, max_count - len(articles))
            }

            if cursor:
                params['pageCursor'] = cursor

            data = self._api_request('list/', params)
            results = data.get('results', [])

            if not results:
                break

            articles.extend(results)
            cursor = data.get('nextPageCursor')

            if not cursor:
                break

        return articles[:max_count]

    def _build_html(self, content, title, author, site_name=None, reading_time=None, published_date=None, summary=None):
        """Build a complete HTML document from article content."""
        from html import escape
        safe_title = escape(title) if title else 'Untitled'

        # Build metadata lines
        meta_parts = []
        if site_name:
            meta_parts.append(escape(site_name))
        if author:
            meta_parts.append(f'By {escape(author)}')

        meta_line = ''
        if meta_parts:
            meta_line = f'<p class="meta">{" · ".join(meta_parts)}</p>'

        # Build secondary metadata (reading time, published date)
        details_parts = []
        if reading_time:
            details_parts.append(f'{reading_time} min read')
        if published_date:
            # Format date nicely if it's an ISO string
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                formatted_date = dt.strftime('%B %d, %Y')
                details_parts.append(formatted_date)
            except (ValueError, AttributeError):
                details_parts.append(escape(str(published_date)))

        details_line = ''
        if details_parts:
            details_line = f'<p class="details">{" · ".join(details_parts)}</p>'

        # Summary/description
        summary_line = ''
        if summary:
            summary_line = f'<p class="summary"><em>{escape(summary)}</em></p>'

        return f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{safe_title}</title>
</head>
<body>
    <article>
        <h1>{safe_title}</h1>
        {meta_line}
        {details_line}
        {summary_line}
        <hr>
        <div class="content">
            {content}
        </div>
    </article>
</body>
</html>'''

    def parse_index(self):
        """Parse the Readwise Reader API and return article index."""
        # Get max articles from recipe options
        max_articles = 20
        if hasattr(self, 'recipe_specific_options'):
            try:
                opt_value = self.get_recipe_specific_option('max_articles')
                if opt_value:
                    max_articles = int(opt_value)
            except (ValueError, AttributeError):
                pass

        self.log(f'Fetching up to {max_articles} articles from Readwise Reader inbox...')

        try:
            articles = self._fetch_articles(max_articles)
        except ValueError as e:
            self.log.error(str(e))
            raise

        if not articles:
            self.log.warn('No articles found in your Readwise Reader inbox.')
            return []

        self.log(f'Found {len(articles)} articles')

        # Sort by saved date (most recent first)
        articles.sort(key=lambda x: x.get('saved_at', x.get('created_at', '')), reverse=True)

        # Create temp directory for article HTML files
        self._temp_dir = tempfile.mkdtemp(prefix='readwise_reader_')
        self.log(f'Using temp directory: {self._temp_dir}')

        # Build the article index
        feed_articles = []
        for idx, article in enumerate(articles):
            title = article.get('title') or 'Untitled'
            url = article.get('source_url') or article.get('url', '')
            author = article.get('author', '')
            summary = article.get('summary', '')
            html_content = article.get('html_content', '')
            site_name = article.get('site_name', '')
            reading_time = article.get('reading_progress', {}).get('time_remaining') if isinstance(article.get('reading_progress'), dict) else None
            # Try reading_time field directly if available
            if not reading_time:
                reading_time = article.get('reading_time')
            published_date = article.get('published_date', '')

            if not html_content:
                self.log.warn(f'Skipping "{title}" - no HTML content available')
                continue

            # Write HTML to temp file
            html = self._build_html(
                html_content, title, author,
                site_name=site_name,
                reading_time=reading_time,
                published_date=published_date,
                summary=summary
            )
            filename = f'article_{idx}.html'
            filepath = os.path.join(self._temp_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html)

            self.log(f'Saved article: {title}')

            feed_articles.append({
                'title': title,
                'url': 'file://' + filepath,
                'author': author,
                'description': summary,
            })

        if not feed_articles:
            self.log.warn('No articles with content found.')
            return []

        self.log(f'Prepared {len(feed_articles)} articles for download')
        return [('Readwise Reader Inbox', feed_articles)]

    def preprocess_html(self, soup):
        """Clean up the HTML before conversion."""
        # Remove any script tags that might have slipped through
        for script in soup.find_all('script'):
            script.decompose()

        # Remove style tags
        for style in soup.find_all('style'):
            style.decompose()

        return soup
